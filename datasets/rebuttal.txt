# Response to Reviewer hTZ2

Thank you for taking the time to review our paper. We sincerely appreciate your valuable comments and suggestions, and we will do our best to address them one by one. We welcome any further discussion or inquiries, and we will respond as soon as possible.

---
Q1. *Using sparse lexicon to generate the video representation is similar to the caption generation in previous methods[1], which also use discrete vocabulary/lexicon/concept to represent one image. More discussions should be made with these methods.*

Thank you for your valuable suggestion. Our lexicon-based method is pretty different from [1] in the following aspects: 1) [1] try to label a set of bounding boxes where each box corresponds to one image, which is limited by the object detector. While our lexicon-based method takes the input as a whole to learn sparse lexicon representation. 2) The above static lexical representation collected from labels is not sensitive to action, and it is difficult to capture action representation from video sequence data. While our method can understand the temporal nature of videos, thereby capturing both visual and action representations simultaneously. 3) [1] leverages multiple instance learning to learn the detectors, which takes text descriptions as the weak supervised signal. While our method employ text descriptions as prompt for semantics of visual-text mapping via masked language modeling.


# Response to Reviewer k6EW

Thank you for taking the time to review our paper. We sincerely appreciate your valuable comments and suggestions, and we will do our best to address them one by one. We welcome any further discussion or inquiries, and we will respond as soon as possible.

---
Q1. *The motivation of the paper seems to be flawed in my opinion. The paper argues that the nearest-neighbor search of dense representation is time consuming, and the proposed method is to overcome this limitation. But there is lots of research about how to speed up the nearest-neighbor search. There were very successful solutions, even with GPU acceleration.*

Thank you for your question. We have already employed the widely used dense vector similarity search library, Faiss, for dense retrieval, following the most common practice as fair as possible. Even so, dense retrieval still faces latency and storage costs, especially in large-scale retrieval scenarios as shown in Table 2 and Table 3. We would like to clarify that we do not aim to speed up nearest-neighbor search, but to replace it with the inverted index, a new retrieval paradigm that does not rely on additional acceleration assistance, ensuring accuracy while minimizing latency and storage consumption.

---
Q2. *The claim in Table 2 and 3 about speed and storage improvements needs to be more carefully investigated. The methods used for comparison are not optimized for speed or storage. I recommend the paper to break down the consumption of storage and speed into different components of the algorithm, before drawing such a strong conclusion. The paper should provide more details about the speed and storage are measured.*

Thank you for your valuable suggestion. As mentioned about, prior dense retrieval methods use Faiss for acceleration, which we compared with as fair as possible. The recommendation to break down the speed and storage consumption into different components of algorithm is indeed valuable. However, it is important to note that such an in-depth analysis may fall outside the scope of the paper or could be subject to practical constraints. The focus of the paper might be on the overall algorithmic improvements rather than specific aspects. Furthermore, we detail speed and storage with the following metrics: ``Index Size'' refers to the storage capacity required to embed all candidate videos, while ``Repr Byte'' indicates the storage requirement for an embedded video. In the lexicon-weighted sparse vector, each activated term incurs a memory overhead of three bytes, comprising two bytes for indexing and one byte for weight. ``QPS'', an acronym for query-per-second, is a commonly used latency metric, which counts the number of query that can be processed in one second during retrieval.



---
Q3. *The bag-of-words process from Eq.(10) converts the sparse vectors into dense vectors? So we start with d-dimensional dense vector, convert it to sparse |V|-dimensional vector, then d-dimensional dense vector again? This detour doesn’t seem to make much sense.Does this lead to information loss? It also goes against the motivation of using sparse vectors to speed up nearest-neighbor search?*

Thank you for your question. 
- From the format, Eq.(10) converts a sparse vector into a dense vector. 
- Actually, we should observe dimensional changes from the entire input: we start with n$\times$d-dimensional dense vector, convert it to sparse |V|-dimensional vector, then d-dimensional dense vector. |n$\times$d|-dimensional dense vector is the representation of image/text input while the d-dimensional dense vector is the compact and low-dimensional bottleneck of the lexicon-importance representation. The bottleneck is implemented through the continuous bag-of-words (CBoW), which derives from the multiplication of lexicon-importance representation (|V|) with the token embeddings (|V|$\times$d). 
- This is the conversion from input to dense information in the dictionary, without considering the loss in input embedding.
- The CBoW component is desinged for lexicon-importance representation learning and will be discarded in the retrieval. We use the top-k of lexicon-weighting representation (ref. Eq.(3)) for inverted index search. We do not aim to speed up nearest-neighbor search, but to replace it with inverted index.

---
Q4. *Why use HowTo100M for video-text retrieval? HowTo100M is designed for pre-training, not for video-text retrieval. There are other datasets dedicated for video-text retrieval, such as DiDeMo, MSVD, LSMDC, etc. Also randomly sampling 10k video-text pairs from HowTo100M for evaluation makes it harder to reproduce the reported results.*

We want to evaluate the model at different levels (10K, 1M) of candidate within the same dataset. The commonly used dataset, such as DiDeMo, MSVD, LSMDC, etc., cannot meet our needs. Thus no previous works take those datasets as large-scale retrieval benchmarks. Following Frozen and Bridging, we use HowTo100M for large-scale video-text retrieval.

---
Q5. *How does the model do jointly pre-training on image-text and video-text datasets, as described in Line 213-214?*

Thank you for your question. Following the most common practice, e.g., MILES, Bridging, we first pre-train our model on the image-text dataset CC3M and video dataset WebVid-2M using 1 frame for 8 epochs. We then pre-train our model on the video dataset WebVid-2M using 4 frames for 4 epochs. More details can be found in Line 220-231.

---
Q6. *The writing of the paper can be improved. Many words are confusing, such as, what is “violent search” in Line 33?*

Thank you for your feedback. What we actually want to express in Line 33 is "brute-force search". We apologize for any confusion we have caused. We will carefully review the paper and make necessary revisions to enhance its clarity, and overall quality.


















# Response to Reviewer xfyT

Thank you for taking the time to review our paper. We sincerely appreciate your valuable comments and suggestions, and we will do our best to address them one by one. We welcome any further discussion or inquiries, and we will respond as soon as possible.

---
Q1. *Novelty is limited. This paper does not differ greatly from LexCLP [1], except for replacing images with videos and adding a learnable vocabulary space. The overall motivation and model structure remain unchanged.*

We appreciate the feedback provided regarding the perceived limited novelty of the paper. While it is true that this work upon the existing LexCLP [1], it is important to emphasize the key differences and contributions this brings.

Firstly, the replacement of images with videos is a significant departure from the original LexCLP approach. Videos introduce temporal dynamics and provide context compared to static images. By incorporating videos, the proposed method aims to capture spatio-temporal information, enabling more comprehensive and accurate understanding of the visual.

Secondly, the addition of a learnable vocabulary space is another notable contribution of this paper. This allows model to adapt and refine its representations based on the specific task at hand. By incorporating a learnable vocabulary, the proposed approach facilitates better alignment between textual and visual modalities, thereby enhancing the overall performance and applicability of the model.

Regarding the overall motivation and model structure, while there may appear to be similarities, it is important to note that this paper introduces notable modifications and improvements in terms of incorporating videos and learnable vocabulary. These modifications contribute to addressing the limitations and expanding the capabilities of the original LexCLP framework.

In conclusion, while there may be some similarities with LexCLP [1], this paper should be recognized for its significant contributions such as the incorporation of videos and the introduction of a learnable vocabulary space. These modifications enhance the model's ability to capture temporal dynamics and align textual and visual information more effectively.

---
Q2. *The method is not video-specific. This paper aims to address the video-text retrieval task, but the model structure design does not consider the characteristic of video.*

Thank you for the valuable suggestion. Although the specific details not have been explicitly stated, it is important to note that building an effective video-text retrieval model inherently requires understanding the unique of video data. Furthermore, it is possible that the paper refers to prior video understanding models or architectures that have been proven to be effective. By leveraging such existing models or techniques, the proposed approach can benefit from established advancements in video-specific methods.

---
Q3. *Lack of adequate experiments in Section 4.2 Common Video-Text Retrieval, (1) In Line 234, the authors mentioned ActivityNet dataset, but they did not conduct any experiments on the ActivityNet. (2) It will be better to perform common video-text retrieval tasks on other datasets (e.g., MSVD, Didemo), which can better verify the generalization of their method.*

Thank you for your feedback. (1) We want to make the statement that the number of candidates in common video-text datasets is really small (e.g., 1k in MSR-VTT and ActivityNet). We apologize for this ambiguity. (2) We report the results on MSVD and Didemo in the table below. As shown in the table, our method has the similar excellent performance on these datasets.

TABLE++++++++++++++

---
Q4. *Lack of adequate experiments in Section 4.3 Large-Scale Video-Text Retrieval. In Line 266, the authors said that "We additionally compare with prior state-of-the-art methods on a really large set (1M candidates) sampled from HowTo100M". But I did not see any experiments about this.*

Thank you for your feedback. The reviewer may have missed some details. As shown in Line 281, we have introduced the large-scale retrieval at the level of millions in Table 3, which reports the comparison with dense-vector based methods for large-scale text-to-video retrieval under zero-shot setting.





# Response to Reviewer VpY6

Thank you for taking the time to review our paper. We sincerely appreciate your valuable comments and suggestions, and we will do our best to address them one by one. We welcome any further discussion or inquiries, and we will respond as soon as possible.


---
W1. *The novelty of the LexVLP is limited, given its substantial similarity to [1]. The high-level idea is identical and both of them are applied in multi-modal retrieval. The main difference is that LexVLP extends the idea to the video-text paradigm. Nevertheless, I did not see any adaptation specific to the video modality.*

I understand the concern raised about the limited novelty of LexVLP and its to the referenced work [1]. While it is true that both approaches are applied in the context of multi-modal retrieval, it is essential to highlight the distinct contributions and advancements made by LexVLP specifically in the video-text paradigm.

The main differentiating factor of LexVLP is its extension to handle videos, which introduces additional challenges and complexities compared to static images. While the high-level idea may share similarities with [1], the adaptation to the video modality is a non-trivial task that requires addressing the temporal dynamics, motion information, and spatio-temporal alignment inherent to video data. The extension to the video-text paradigm itself is a significant contribution. It enables the model to leverage the contextual information present in videos, enhancing the overall capability of multi-modal retrieval systems. By considering both visual and textual cues in the video modality, LexVLP offers new insights and opportunities for applications that require understanding and retrieving video-based information.


---
W2. *The comparisons are made between LexVLP and existing approaches in terms of retrieval efficiency and performance. This ignores a factor, which is the computation for extracting the features, since performance may be directly influenced by the model scale and input resolution, etc.*

Thank you for the comment. Considering in practical application, the computation for extracting the features can be completed in advance. Therefore, following most previous works, we do not include this part in the overall retrieval time. We have made an effort to ensure a fair comparison by utilizing commonly used model scale and input resolution in our study. Specifically, we have adopted the widely used BERT-base as language encoder and VIT as visual encoder wherein the frame resolution is 224 × 224 with a patch size 16.


---
W3. *Some of the expressions in the manuscript lack explanation. For example, what does the $\nabla$ mark indicate in Eq.18?*

Thank you for your careful review. $\nabla$ is the partial differential symbol in solving gradients. We will carefully review the paper and make necessary revisions to enhance its clarity.

---
W4. *Overall, the writing or the organization of the paper can be improved. I have many questions regarding the manuscript since I couldn't find the answers in the manuscript (see below). I would appreciate it if the authors could provide more details on the raised questions. Following the above weakness as well as the first one, there are a lot of descriptions in the manuscript that are based on LexLIP (L93, L134, L227, L229, L233, L252...). I would expect more carefulness for a manuscript submitted to a top-level conference.*

Thank you for the kind reminder. The similarity between images and videos may lead to similar descriptions in visual issues, the manuscript has clearly emphasize the novel contributions and advancements. These can include but are not limited to the incorporation of videos, the utilization of a learnable vocabulary, or any other enhancements specific to the current research.



---
Q1. *Usually, sparsity brings advantages in speed but disadvantages in performance. Could you provide some intuitive explanations (or better empirical ones) on how LexVLP can outperform dense representations based approaches?*

Thank you for the question. While it is generally true that sparsity can lead to advantages in terms of speed but potential disadvantages in performance, LexVLP provides some intuitive explanations for how it can outperform dense representations-based approaches:

- Effective information selection: LexVLP leverages learnable vocabulary space, allowing the model to adaptively select and focus on relevant information. This selective mechanism enables the model to important visual and textual cues, effectively capturing the most relevant and discriminative features. By selectively attending to crucial, LexVLP can potentially achieve higher performance compared to dense representations that might include irrelevant or noisy features.

- Context understanding: LexVLP benefits from the temporal dynamics and contextual information present in videos. LexVLP's sparse representation of videos is based on the entire video. This contextual understanding helps the model to capture dependencies, relationships, and context-specific information, leading to improved performance compared to dense representations that lack temporal context.

---
Q2. *Could you give a detailed explanation on 'top-k sparsify'?*

We are happy to give an example for detail explanation: the sparse vector is a |V|-dimensional vector, Assuming the vector is [3,0,7,0,...,0] where the value means the weight of the token, which can be identified by the index (assuming as [apple, car, man, sad,..., zoom]). The top-2 sparsify can be [7,3] then the corresponding tokens are [man, apple]. 

---
Q3. *I can understand that sparsity could reduce the computation for comparing different samples, but I am not quite sure how it reduces the storage requirement for an embedded video, and why the 'Repr Byte' is 'up to' a certain repr byte. Could you provide more details on this?*

Taking the sparse vector [3, 0 7, 0, ..., 0] as an example, let's assume there are only two non-zero numbers. When we apply top-8 sparsity, the 'Repr Byte' is at most 24 Repr Byte, but this is only the upper limit when all sparse have 8 non-zero numbers. However, not all sparse vectors possess top-8 valuable numbers as shown in the example. Hence, the actual 'Repr Byte' is lower than 24.

---
Q4. *Why does static vocabulary bring a huge amount of parameters for the vocab projection head, and why the learnable vocabulary can address this problem?*

Thank you for the question. the dimension of the static vocab projection head is |V|$\times$d where |V| is the whole vocabulary. While the learnable vocabulary is reducible by improving the utilization rate of dictionaries.

---
Q5. *It is observed in Table 4 that NCE plays an important role in improving performance, while the lexicon-based MLM for both text and video has limited influence. I wonder whether it is possible to remove the lexicon-based MLM, and if so, how is the performance?*

Thank you for the constructive comment. We are already aware of this phenomenon and remove the lexicon-based MLM in the training. However, we found it difficult to converge. This may be due to the large size of the sparse space, making it difficult to directly map from dense space to sparse space without lexicon bottleneck (i.e., lexicon-based MLM) to aid in understanding.

---
Q6. *To my best knowledge, the text data in the HowTo100M dataset is generated by performing ASR on the input video. Is it valid or reasonable to use such a dataset for retrieval experiments?*


We want to evaluate the model at different levels (10K, 1M) of candidate within the same dataset. The commonly used dataset, such as DiDeMo, MSVD, LSMDC, etc., cannot meet our needs. Thus no previous works take those datasets as large-scale retrieval benchmarks. Following Frozen and Bridging, we use HowTo100M for large-scale video-text retrieval.

---
Q7. *What is the difference in the experimental settings between Table 2 and Table 3?*
The difference is the number of retrieval candidate. Table 2 have 10k candidates sampled from 


# Response to Reviewer AEL3

Thank you for taking the time to review our paper. We sincerely appreciate your valuable comments and suggestions, and we will do our best to address them one by one. We welcome any further discussion or inquiries, and we will respond as soon as possible.

---
Q1. **


---
Q2. **


---
Q3. **


---
Q4. **


---
Q5. **






